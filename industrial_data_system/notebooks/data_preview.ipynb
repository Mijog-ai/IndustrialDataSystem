{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data File Preview - ASC and TDMS Files\n",
    "\n",
    "This notebook lets you preview how your ASC or TDMS files would look when converted to Parquet format.\n",
    "No files are written - just viewing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_asc_file(file_name):\n",
    "    \"\"\"Load and process ASC file.\"\"\"\n",
    "    # Read the ASC file\n",
    "    df = pd.read_csv(file_name, delim_whitespace=True)\n",
    "    \n",
    "    # Fill NaN values with 0.0\n",
    "    df = df.fillna(0.0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_tdms_file(file_name):\n",
    "    \"\"\"Load TDMS file.\"\"\"\n",
    "    from nptdms import TdmsFile\n",
    "\n",
    "    with TdmsFile.open(file_name) as tdms_file:\n",
    "        # Get all groups in the file\n",
    "        groups = tdms_file.groups()\n",
    "\n",
    "        # Create a dictionary to store data from all groups\n",
    "        data_dict = {}\n",
    "\n",
    "        for group in groups:\n",
    "            for channel in group.channels():\n",
    "                channel_name = f\"{group.name}/{channel.name}\"\n",
    "                data = channel[:]\n",
    "                data_dict[channel_name] = data\n",
    "\n",
    "        # Find the maximum length of data\n",
    "        max_length = max(len(data) for data in data_dict.values())\n",
    "\n",
    "        # Pad shorter arrays with NaN\n",
    "        for key in data_dict:\n",
    "            if len(data_dict[key]) < max_length:\n",
    "                pad_length = max_length - len(data_dict[key])\n",
    "                data_dict[key] = np.pad(\n",
    "                    data_dict[key],\n",
    "                    (0, pad_length),\n",
    "                    'constant',\n",
    "                    constant_values=np.nan\n",
    "                )\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data_dict)\n",
    "        df = df.fillna(0.0)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Enter Your File Path\n",
    "\n",
    "Change the path below to point to your ASC or TDMS file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ‘‡ CHANGE THIS to your file path\n",
    "input_file = \"source_folder/your_file.asc\"  # or .tdms\n",
    "\n",
    "# Or use this format if you want to specify a complete path:\n",
    "# input_file = \"/full/path/to/your/file.asc\"\n",
    "# input_file = \"/full/path/to/your/file.tdms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Preview the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect file type and load accordingly\n",
    "file_path = Path(input_file)\n",
    "file_extension = file_path.suffix.lower()\n",
    "\n",
    "print(f\"Loading file: {input_file}\")\n",
    "print(f\"File type: {file_extension}\")\n",
    "print()\n",
    "\n",
    "if file_extension in ['.asc', '.sc']:\n",
    "    df = load_and_process_asc_file(input_file)\n",
    "    print(\"âœ“ Loaded ASC file\")\n",
    "elif file_extension == '.tdms':\n",
    "    df = load_and_process_tdms_file(input_file)\n",
    "    print(\"âœ“ Loaded TDMS file\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported file type: {file_extension}. Use .asc, .sc, or .tdms\")\n",
    "\n",
    "print(f\"âœ“ Shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: View Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show column names\n",
    "print(\"Column names:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nTotal columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data types\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preview the Data (First Few Rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Check for Potential Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for columns with all zeros\n",
    "all_zero_cols = []\n",
    "for col in df.columns:\n",
    "    if (df[col].fillna(0) == 0.0).all():\n",
    "        all_zero_cols.append(col)\n",
    "\n",
    "if all_zero_cols:\n",
    "    print(f\"âš ï¸ Found {len(all_zero_cols)} columns with all zeros:\")\n",
    "    print(all_zero_cols)\n",
    "else:\n",
    "    print(\"âœ“ No all-zero columns found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unnamed or invalid columns\n",
    "invalid_cols = []\n",
    "for col in df.columns:\n",
    "    col_str = str(col).strip()\n",
    "    if (not col_str or col_str == '' or \n",
    "        col_str.startswith('Unnamed:') or \n",
    "        col_str.lower() == 'nan' or pd.isna(col)):\n",
    "        invalid_cols.append(col)\n",
    "\n",
    "if invalid_cols:\n",
    "    print(f\"âš ï¸ Found {len(invalid_cols)} columns with invalid names:\")\n",
    "    print(invalid_cols)\n",
    "else:\n",
    "    print(\"âœ“ All columns have valid names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: View Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Check for Missing Values (Before fillna was applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Since both functions apply fillna(0.0), there should be no missing values\n",
    "# But we can check anyway\n",
    "missing = df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "\n",
    "if len(missing) > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"âœ“ No missing values (all NaNs were filled with 0.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: View Last Few Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show last 10 rows\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: View Sample of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show random sample of 10 rows\n",
    "df.sample(min(10, len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Check Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show memory usage\n",
    "memory_usage = df.memory_usage(deep=True).sum() / 1024**2  # Convert to MB\n",
    "print(f\"DataFrame memory usage: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Columns - Keep Only Specific Columns\n",
    "\n",
    "This section shows you how to keep only the columns you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Select specific columns by name (RECOMMENDED)\n",
    "# List the exact columns you want to keep\n",
    "columns_to_keep = [\n",
    "    'Messzeit[s]',\n",
    "    'Pressure [bar]',\n",
    "    'Flow [L/min]',\n",
    "    'Leak [L/min]',\n",
    "    'Torque [Nm]'\n",
    "]\n",
    "\n",
    "# Create filtered dataframe\n",
    "df_filtered = df[columns_to_keep]\n",
    "\n",
    "print(f\"Original DataFrame: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"Filtered DataFrame: {df_filtered.shape[0]} rows Ã— {df_filtered.shape[1]} columns\")\n",
    "print(f\"\\nRemoved {df.shape[1] - df_filtered.shape[1]} columns\")\n",
    "print(f\"\\nKept columns: {df_filtered.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the filtered data\n",
    "print(\"First 10 rows of filtered data:\")\n",
    "df_filtered.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Drop unwanted columns (Alternative approach)\n",
    "# List the columns you want to REMOVE\n",
    "columns_to_remove = [\n",
    "    'Speed [rpm]',\n",
    "    'LS [bar]',\n",
    "    'Housing [bar]',\n",
    "    'TempSaug [Â°C]',\n",
    "    'TempLeak [Â°C]'\n",
    "]\n",
    "\n",
    "# Create filtered dataframe by dropping columns\n",
    "df_filtered_alt = df.drop(columns=columns_to_remove, errors='ignore')\n",
    "\n",
    "print(f\"Alternative method result: {df_filtered_alt.shape[0]} rows Ã— {df_filtered_alt.shape[1]} columns\")\n",
    "print(f\"Columns: {df_filtered_alt.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Filter by pattern (if you have many similar columns)\n",
    "# Keep columns that contain certain keywords\n",
    "keywords = ['Messzeit', 'Pressure', 'Flow', 'Leak', 'Torque']\n",
    "\n",
    "# Find columns matching any keyword\n",
    "matching_columns = [col for col in df.columns if any(keyword in col for keyword in keywords)]\n",
    "\n",
    "df_filtered_pattern = df[matching_columns]\n",
    "\n",
    "print(f\"Pattern-based filtering result: {df_filtered_pattern.shape[0]} rows Ã— {df_filtered_pattern.shape[1]} columns\")\n",
    "print(f\"Columns: {df_filtered_pattern.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare statistics before and after filtering\n",
    "print(\"Statistics for filtered data:\")\n",
    "df_filtered.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Filtered Data (Optional)\n",
    "\n",
    "If you want to save the filtered data to a new file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify the path to save filtered data\n",
    "\n",
    "# Save as Parquet (recommended for large files)\n",
    "# output_path = \"filtered_data.parquet\"\n",
    "# df_filtered.to_parquet(output_path, index=False)\n",
    "# print(f\"âœ“ Saved filtered data to {output_path}\")\n",
    "\n",
    "# Or save as CSV\n",
    "# output_path = \"filtered_data.csv\"\n",
    "# df_filtered.to_csv(output_path, index=False)\n",
    "# print(f\"âœ“ Saved filtered data to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Find Columns by Partial Name\n",
    "\n",
    "If you're not sure of the exact column names, use this to search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for columns containing a specific word\n",
    "search_term = \"Temp\"  # Change this to search for different terms\n",
    "\n",
    "matching = [col for col in df.columns if search_term.lower() in col.lower()]\n",
    "\n",
    "if matching:\n",
    "    print(f\"Columns containing '{search_term}':\")\n",
    "    for col in matching:\n",
    "        print(f\"  - {col}\")\n",
    "else:\n",
    "    print(f\"No columns found containing '{search_term}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
