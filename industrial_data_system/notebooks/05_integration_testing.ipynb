{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration Testing\n",
    "\n",
    "This notebook performs end-to-end integration testing of the Industrial Data System components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Add parent directories to path for module imports\n",
    "notebook_dir = Path(os.path.abspath(''))\n",
    "project_root = notebook_dir.parent.parent  # Go up two levels from notebooks/\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test All Import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing module imports...\\n\")\n",
    "test_results = []\n",
    "\n",
    "# Test imports\n",
    "imports_to_test = [\n",
    "    ('industrial_data_system.utils.asc_utils', ['load_and_process_asc_file', 'convert_asc_to_parquet']),\n",
    "    ('industrial_data_system.core.config', ['get_config']),\n",
    "    ('industrial_data_system.core.db_manager', ['DatabaseManager']),\n",
    "]\n",
    "\n",
    "for module_name, items in imports_to_test:\n",
    "    try:\n",
    "        if items:\n",
    "            exec(f\"from {module_name} import {', '.join(items)}\")\n",
    "        else:\n",
    "            exec(f\"import {module_name}\")\n",
    "        print(f\"✓ {module_name}\")\n",
    "        test_results.append((module_name, True, None))\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {module_name}: {str(e)}\")\n",
    "        test_results.append((module_name, False, str(e)))\n",
    "\n",
    "success_count = sum(1 for _, success, _ in test_results if success)\n",
    "print(f\"\\nImport Results: {success_count}/{len(test_results)} successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Data Pipeline (Load → Process → Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Normal_data_to_fft import (\n",
    "    load_and_process_asc_file,\n",
    "    extract_pump_channels,\n",
    "    process_all_channels_from_asc\n",
    ")\n",
    "\n",
    "print(\"Testing complete data pipeline...\\n\")\n",
    "\n",
    "# Step 1: Load\n",
    "start_time = time.time()\n",
    "test_file = '../Tests/Data/V24-2025__0011_2.ASC'\n",
    "\n",
    "if os.path.exists(test_file):\n",
    "    df = load_and_process_asc_file(test_file)\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"✓ Step 1 - Load: {df.shape[0]} samples loaded in {load_time:.2f}s\")\n",
    "    \n",
    "    # Step 2: Extract channels\n",
    "    start_time = time.time()\n",
    "    data = extract_pump_channels(df)\n",
    "    extract_time = time.time() - start_time\n",
    "    print(f\"✓ Step 2 - Extract: {len(data)} channels extracted in {extract_time:.2f}s\")\n",
    "    \n",
    "    # Step 3: FFT Transform\n",
    "    start_time = time.time()\n",
    "    freqs, fft_features, feature_matrix = process_all_channels_from_asc(\n",
    "        data, window_size=1024, overlap=0.5, max_freq=500.0\n",
    "    )\n",
    "    fft_time = time.time() - start_time\n",
    "    print(f\"✓ Step 3 - FFT Transform: {feature_matrix.shape} features in {fft_time:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nTotal pipeline time: {load_time + extract_time + fft_time:.2f}s\")\n",
    "    pipeline_success = True\n",
    "else:\n",
    "    print(f\"✗ Test file not found: {test_file}\")\n",
    "    print(\"Creating synthetic data for testing...\")\n",
    "    \n",
    "    # Create synthetic data\n",
    "    t = np.linspace(0, 50, 5000)\n",
    "    data = {\n",
    "        'Messzeit[s]': t,\n",
    "        'Pressure [bar]': 50 + 5*np.sin(2*np.pi*0.5*t) + np.random.normal(0, 0.5, len(t)),\n",
    "        'Flow [L/min]': 100 + 10*np.sin(2*np.pi*0.3*t) + np.random.normal(0, 1, len(t)),\n",
    "        'Speed [rpm]': 1500 + 50*np.sin(2*np.pi*0.2*t) + np.random.normal(0, 5, len(t)),\n",
    "    }\n",
    "    \n",
    "    freqs, fft_features, feature_matrix = process_all_channels_from_asc(\n",
    "        data, window_size=1024, overlap=0.5, max_freq=500.0\n",
    "    )\n",
    "    print(\"✓ Synthetic data pipeline complete\")\n",
    "    pipeline_success = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Database Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from industrial_data_system.core.db_manager import DatabaseManager\n",
    "\n",
    "print(\"Testing database operations...\\n\")\n",
    "\n",
    "try:\n",
    "    # Initialize database\n",
    "    db_manager = DatabaseManager()\n",
    "    print(f\"✓ Database initialized\")\n",
    "    \n",
    "    # Test model registry operations\n",
    "    models = db_manager.get_all_models()\n",
    "    print(f\"✓ Model registry accessible: {len(models) if models else 0} models found\")\n",
    "    \n",
    "    db_success = True\n",
    "except Exception as e:\n",
    "    print(f\"✗ Database error: {str(e)}\")\n",
    "    db_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from industrial_data_system.core.config import get_config\n",
    "\n",
    "print(\"Testing configuration management...\\n\")\n",
    "\n",
    "try:\n",
    "    config = get_config()\n",
    "    print(f\"✓ Configuration loaded\")\n",
    "    \n",
    "    # Display some config values\n",
    "    if hasattr(config, '__dict__'):\n",
    "        print(\"\\nConfiguration values:\")\n",
    "        for key, value in list(config.__dict__.items())[:5]:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    config_success = True\n",
    "except Exception as e:\n",
    "    print(f\"✗ Configuration error: {str(e)}\")\n",
    "    config_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test File Format Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from industrial_data_system.utils.asc_utils import (\n",
    "    load_and_process_asc_file,\n",
    "    load_and_process_csv_file,\n",
    "    load_and_process_tdms_file\n",
    ")\n",
    "\n",
    "print(\"Testing file format support...\\n\")\n",
    "\n",
    "format_support = {\n",
    "    'ASC': True,  # Already tested\n",
    "    'CSV': False,\n",
    "    'TDMS': False\n",
    "}\n",
    "\n",
    "# Test CSV loading (if CSV file exists)\n",
    "try:\n",
    "    # This is just testing if the function exists and is callable\n",
    "    if callable(load_and_process_csv_file):\n",
    "        print(\"✓ CSV loader available\")\n",
    "        format_support['CSV'] = True\n",
    "except Exception as e:\n",
    "    print(f\"✗ CSV loader: {str(e)}\")\n",
    "\n",
    "# Test TDMS loading\n",
    "try:\n",
    "    if callable(load_and_process_tdms_file):\n",
    "        print(\"✓ TDMS loader available\")\n",
    "        format_support['TDMS'] = True\n",
    "except Exception as e:\n",
    "    print(f\"✗ TDMS loader: {str(e)}\")\n",
    "\n",
    "print(f\"\\nSupported formats: {', '.join([k for k, v in format_support.items() if v])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running performance benchmarks...\\n\")\n",
    "\n",
    "# Benchmark data sizes\n",
    "data_sizes = [1000, 5000, 10000]\n",
    "benchmark_results = []\n",
    "\n",
    "for size in data_sizes:\n",
    "    # Create synthetic data\n",
    "    t = np.linspace(0, size/100, size)\n",
    "    test_data = {\n",
    "        'Messzeit[s]': t,\n",
    "        'Channel1': np.sin(2*np.pi*t) + np.random.normal(0, 0.1, size),\n",
    "        'Channel2': np.cos(2*np.pi*t) + np.random.normal(0, 0.1, size),\n",
    "        'Channel3': np.sin(4*np.pi*t) + np.random.normal(0, 0.1, size),\n",
    "    }\n",
    "    \n",
    "    # Benchmark FFT processing\n",
    "    start_time = time.time()\n",
    "    freqs, fft_features, feature_matrix = process_all_channels_from_asc(\n",
    "        test_data, window_size=min(512, size//4), overlap=0.5, max_freq=100.0\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    benchmark_results.append((size, elapsed))\n",
    "    print(f\"  Size {size:6d}: {elapsed:.3f}s ({size/elapsed:.0f} samples/s)\")\n",
    "\n",
    "# Plot benchmark results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sizes, times = zip(*benchmark_results)\n",
    "ax.plot(sizes, times, 'o-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Data Size (samples)', fontsize=12)\n",
    "ax.set_ylabel('Processing Time (seconds)', fontsize=12)\n",
    "ax.set_title('FFT Processing Performance', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Analyzing memory usage...\\n\")\n",
    "\n",
    "# Check memory usage of main data structures\n",
    "if 'df' in locals():\n",
    "    df_memory = df.memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "    print(f\"  DataFrame: {df_memory:.2f} MB\")\n",
    "\n",
    "if 'feature_matrix' in locals():\n",
    "    matrix_memory = feature_matrix.nbytes / 1024**2  # MB\n",
    "    print(f\"  Feature Matrix: {matrix_memory:.2f} MB\")\n",
    "\n",
    "if 'fft_features' in locals():\n",
    "    fft_memory = sum(v.nbytes for v in fft_features.values()) / 1024**2  # MB\n",
    "    print(f\"  FFT Features: {fft_memory:.2f} MB\")\n",
    "\n",
    "# Total memory footprint estimate\n",
    "total_memory = 0\n",
    "if 'df_memory' in locals():\n",
    "    total_memory += df_memory\n",
    "if 'matrix_memory' in locals():\n",
    "    total_memory += matrix_memory\n",
    "if 'fft_memory' in locals():\n",
    "    total_memory += fft_memory\n",
    "\n",
    "print(f\"\\n  Total estimated memory: {total_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Handling Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing error handling...\\n\")\n",
    "\n",
    "error_tests = []\n",
    "\n",
    "# Test 1: Non-existent file\n",
    "try:\n",
    "    load_and_process_asc_file('nonexistent_file.asc')\n",
    "    error_tests.append(('Non-existent file', False, 'Should have raised error'))\n",
    "except Exception as e:\n",
    "    error_tests.append(('Non-existent file', True, str(type(e).__name__)))\n",
    "    print(f\"✓ Non-existent file handling: {type(e).__name__}\")\n",
    "\n",
    "# Test 2: Invalid data\n",
    "try:\n",
    "    invalid_data = {'time': [1, 2, 3]}  # Missing required structure\n",
    "    process_all_channels_from_asc(invalid_data)\n",
    "    error_tests.append(('Invalid data', False, 'Should have raised error'))\n",
    "except Exception as e:\n",
    "    error_tests.append(('Invalid data', True, str(type(e).__name__)))\n",
    "    print(f\"✓ Invalid data handling: {type(e).__name__}\")\n",
    "\n",
    "# Test 3: Empty data\n",
    "try:\n",
    "    empty_data = {'Messzeit[s]': np.array([])}\n",
    "    process_all_channels_from_asc(empty_data)\n",
    "    error_tests.append(('Empty data', False, 'Should have raised error'))\n",
    "except Exception as e:\n",
    "    error_tests.append(('Empty data', True, str(type(e).__name__)))\n",
    "    print(f\"✓ Empty data handling: {type(e).__name__}\")\n",
    "\n",
    "print(f\"\\nError handling: {sum(1 for _, success, _ in error_tests if success)}/{len(error_tests)} tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integration Test Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INTEGRATION TEST SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect all test results\n",
    "test_categories = [\n",
    "    ('Module Imports', success_count, len(test_results)),\n",
    "    ('Data Pipeline', 1 if pipeline_success else 0, 1),\n",
    "    ('Database Operations', 1 if db_success else 0, 1),\n",
    "    ('Configuration', 1 if config_success else 0, 1),\n",
    "    ('File Format Support', sum(format_support.values()), len(format_support)),\n",
    "    ('Error Handling', sum(1 for _, s, _ in error_tests if s), len(error_tests)),\n",
    "]\n",
    "\n",
    "total_passed = sum(passed for _, passed, _ in test_categories)\n",
    "total_tests = sum(total for _, _, total in test_categories)\n",
    "\n",
    "print(f\"\\nTest Results by Category:\")\n",
    "print(\"-\" * 70)\n",
    "for category, passed, total in test_categories:\n",
    "    percentage = (passed/total*100) if total > 0 else 0\n",
    "    status = \"✓\" if passed == total else \"⚠\" if passed > 0 else \"✗\"\n",
    "    print(f\"{status} {category:30s}: {passed:2d}/{total:2d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "overall_percentage = (total_passed/total_tests*100) if total_tests > 0 else 0\n",
    "print(f\"\\nOverall Test Results: {total_passed}/{total_tests} ({overall_percentage:.1f}%)\")\n",
    "\n",
    "if overall_percentage >= 90:\n",
    "    print(\"\\n✓ System Status: EXCELLENT - All systems operational\")\n",
    "elif overall_percentage >= 70:\n",
    "    print(\"\\n⚠ System Status: GOOD - Minor issues detected\")\n",
    "elif overall_percentage >= 50:\n",
    "    print(\"\\n⚠ System Status: FAIR - Some components need attention\")\n",
    "else:\n",
    "    print(\"\\n✗ System Status: POOR - Significant issues detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Performance summary\n",
    "if benchmark_results:\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    avg_time_per_1k = np.mean([t/(s/1000) for s, t in benchmark_results])\n",
    "    print(f\"  Average processing time: {avg_time_per_1k:.3f}s per 1000 samples\")\n",
    "\n",
    "if 'total_memory' in locals():\n",
    "    print(f\"\\nMemory Usage:\")\n",
    "    print(f\"  Total memory footprint: {total_memory:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Test Report (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test report to file\n",
    "report_file = 'integration_test_report.txt'\n",
    "\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"INTEGRATION TEST REPORT\\n\")\n",
    "    f.write(f\"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Test Results by Category:\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    for category, passed, total in test_categories:\n",
    "        percentage = (passed/total*100) if total > 0 else 0\n",
    "        f.write(f\"{category:30s}: {passed:2d}/{total:2d} ({percentage:5.1f}%)\\n\")\n",
    "    \n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(f\"\\nOverall: {total_passed}/{total_tests} ({overall_percentage:.1f}%)\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"\\n✓ Test report saved to: {report_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
